<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="en">
<head>
<META http-equiv="Content-Type" content="text/html; charset=UTF-8">
<!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      -->
<title>DeepMIB - Network panel</title>
<meta name="generator" content="MATLAB 23.2">
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/">
<meta name="DC.date" content="2024-03-07">
<meta name="DC.source" content="ug_gui_menu_tools_deeplearning_network.m">
<style>
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,my-a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a,my-a { color:#005fce; text-decoration:none; }
my-a:hover { cursor: pointer; }
a:hover,my-a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; }

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:160%; padding: 20px; }

pre { font-size:12px; }
code { font-size: 1.15em; }
pre { margin:0px 0px 15px; overflow-x:auto; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 15px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }
span.typesection { color:#A0522D }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }




.dropdown { font-family: monospace; border: 1px solid #aaa; border-radius: 0.2em; background-color: #fff; padding: 0.1em 0.4em; font-size: 1em; }
.kbd { font-family: monospace; border: 1px solid #aaa; -moz-border-radius: 0.2em; -webkit-border-radius: 0.2em; border-radius: 0.2em; -moz-box-shadow: 0.1em 0.2em 0.2em #ddd; -webkit-box-shadow: 0.1em 0.2em 0.2em #ddd; box-shadow: 0.1em 0.2em 0.2em #ddd; background-color: #f9f9f9; background-image: -moz-linear-gradient(top, #eee, #f9f9f9, #eee); background-image: -o-linear-gradient(top, #eee, #f9f9f9, #eee); background-image: -webkit-linear-gradient(top, #eee, #f9f9f9, #eee); background-image: linear-gradient([[:Template:Linear-gradient/legacy]], #eee, #f9f9f9, #eee); padding: 0.1em 0.4em; font-family: inherit; font-size: 1em; }
.h3 { color: #E65100; font-size: 12px; font-weight: bold; }
.code { font-family: monospace; font-size: 10pt; background: #eee; padding: 1pt 3pt; }

#tooltiptext {
  visibility: hidden;
  padding: 5px 10px;
  font-size: 75%;
  line-height:110%;
  text-align: center;
  background-color: black;
  color: #ddd;
  border-radius: 6px;
  position: fixed;
  bottom: 11px;
  right: 62px;
  z-index: 2;
}
#tooltiptext::after {
  content: " ";
  position: absolute;
  top: 50%;
  left: 100%;
  margin-top: -5px;
  border-width: 5px;
  border-style: solid;
  border-color: transparent transparent transparent black;
}
.tooltip:hover #tooltiptext {
  visibility: visible;
}
#return-link {
    position: fixed;
    bottom: 10px;
    right: 10px;
    overflow: visible;
    font-size:120%;
    background: rgba(0, 0, 0, 0.75);
    border-style: solid;
    border-width: 3pt;
    border-color: #202020;
    border-radius: 4px;
    cursor: pointer;
    }
#return-link > p { padding:3px; margin:0; color:#C0C0C0;}
.MATLAB-Help {
width: 100%;
margin-bottom: 12px;
border: 1px solid #ccc;
border-right: none;
border-bottom: none;
font-size: 96%;
line-height: 1.4;
table-layout: fixed;
overflow:hidden;}

.MATLAB-Help > thead > tr > th {
padding: 6px 5px;
border: none;
border-right: 1px solid #ccc;
border-bottom: 1px solid #ccc;
background: #F2F2F2;
color: #000;
font-weight: bold;
text-align: left;
vertical-align: middle;}

.MATLAB-Help td{padding: 5px 5px;
border: none;
border-right: 1px solid #ccc;
border-bottom: 1px solid #ccc;
vertical-align: middle;}

.language-matlab { line-height:135% }

.collapse-link {float:right; line-height:200%; padding-left:10px; margin:0}


details > summary,
.details-div {
  padding: 8px 20px;
  border-style: solid;
  border-width: 1.2pt;
  border-color: #E0E0E0;
}
details > summary {
  border-radius:6px 6px 0 0;
  background-color: #F2F2F2;
  cursor: pointer;
}
.details-div {
  border-top-style: none;
  border-radius: 0 0 6px 6px;
}
.image-fit-svg,
.image-fit {
    max-width:  95%;
    max-height: 100%;
    margin:     auto;
}
.image-fit-svg{ padding:0px; max-width:500px; }
details > img.image-fit-svg{ padding: 0px 0px 10px; }
@media (max-width: 580px) {
  .image-fit-svg { max-width: 95%; }
}
.pretty-link  { color:#001188 !important; }
</style>
<style id="dark-theme">
    h2, h3       { color: #B0B0B0; }
    html body    { background-color: #101010; color: #B0B0B0; }
    .pretty-link { color: #C46313 !important; }
    a, a:visited, my-a  { color: #C46313 }
    a:hover, my-a:hover { color: orange; }
    details > summary,
    .details-div      { border-color:     #505050; }
    details > summary { background-color: #202020; }
    pre.codeinput     { border-width: 1.2pt; border-color:#001B33; background:#001129; color:#F0F0F0; }
    pre.codeoutput    { color:#A5A5A5; }
    span.keyword      { color:#FF9D00; }
    span.comment      { color:#808080; }
    span.string       { color:#3AD900; }
    span.untermstring { color:#FFEE80; }
    span.syscmd       { color:#CCCCCC; }
    .MATLAB-Help, .MATLAB-Help > thead > tr > th, .MATLAB-Help td { border-color:#505050; }
    .MATLAB-Help > thead > tr > th { background: #202020; color: #B0B0B0; }
    .summary-sub-heading { color:#909090; }
    .show-if-light    { display:none }
</style>
<style id="hide-dark">
     .show-if-dark { display:none }
</style>

<style id="anchor-offsets">
    h2::before, a[id]::before{
    content: "";
    display: block;
    height: 100px;
    margin: -100px 0 0;
    visibility: hidden;
    width:10%;
    z-index: -1;
}
</style>

<script>
          var returnElem = null;
          var skipCheck  = false;

          function hide_back_link()
          {
              returnButton.style.display = "none";
              try{
                 window.removeEventListener("scroll", update_back_position, true);
                 window.removeEventListener("resize", update_back_position, true);
                 parent.window.removeEventListener("scroll", update_back_position, true);
                 parent.window.removeEventListener("resize", update_back_position, true);}
              catch(e){}
          }

          function get_offset(element)
          {
              if (!element.getClientRects().length){ return { top: 0, left: 0 }; }
              var rect = element.getBoundingClientRect();
              var win  = element.ownerDocument.defaultView;
              return ( {top:  rect.top  + win.pageYOffset,
                        left: rect.left + win.pageXOffset} );
          }

          function jump_to()
          {
              var clickedElem = event.target;
              var clickedID   = clickedElem.closest("span");
              if (clickedID){
                clickedID = clickedID.getAttribute("id");
                if (clickedID.localeCompare("jump-close")===0) { return };}
              clickedID = clickedElem.closest("div").getAttribute("id");
              if (clickedID && clickedID.localeCompare("return-link")===0)
              {
                  if (returnElem)
                  {
                      event.preventDefault();
                      hide_back_link();
                      returnElem.scrollIntoView();
                      if (contentDiv.getAttribute("data-isHelpBrowser")){
                         contentDiv.scrollTop = contentDiv.scrollTop-100; }
                      if (contentDiv.getAttribute("data-isMATLABCentral")){
                         parent.window.scrollBy(0,-100)}
                      returnElem = null;
                  }
              }
              else
              {
                  var href = clickedElem.closest("my-a").getAttribute("href");
                  if ( href && href[0] == "#" )
                  {
                     var target = document.getElementById(href.substring(1));
                     var enclosingBox = target;
                     while ( enclosingBox )
                     {
                        prevBox      = enclosingBox;
                        enclosingBox = enclosingBox.closest("details");
                        if ( enclosingBox===prevBox ){
                           enclosingBox = enclosingBox.parentElement
                           if ( enclosingBox ) { enclosingBox = enclosingBox.closest("details"); }  }
                        if (enclosingBox && !enclosingBox.open) { open_details(enclosingBox.id) }
                     }
                     if (target){
                        event.preventDefault();
                        target.scrollIntoView(); }
                     var nextElem = target.nextElementSibling;
                     var nextNode = target.nextSibling;
                     while ( nextNode && nextNode.nodeType==Node.TEXT_NODE && nextNode.data.trim().length == 0 ){
                        nextNode = nextNode.nextSibling;}
                     if ( nextElem && nextElem===nextNode && nextElem.localName.localeCompare("details")===0 && !nextElem.open){
                        open_details(nextElem.id);}
                  }
                  else { return }
                  if (!contentDiv.getAttribute("data-isHelpBrowser"))
                  {
                      update_back_position();
                      returnButton.style.display = "block";
                      var linkTop   = clickedElem.offsetTop;
                      var targetTop = target.offsetTop;
                      if (targetTop>linkTop){
                          document.getElementById("down").style.display = "none";
                          document.getElementById("up").style.display   = "inline"; }
                      else{
                          document.getElementById("up").style.display   = "none";
                          document.getElementById("down").style.display = "inline"; }
                      returnElem = clickedElem;
                  }
              }
          }

          function open_details(detailsID)
          {
              var details  = document.getElementById(detailsID);
              skipCheck    = true;
              state_check(details.id);
              details.open = true;
          }

          function update_back_position()
          {
              try
              {
                  window.addEventListener("scroll", update_back_position, true);
                  window.addEventListener("resize", update_back_position, true);
                  var scrollPos;
                  if (in_iFrame())
                  {
                      parent.window.addEventListener("scroll", update_back_position, true);
                      parent.window.addEventListener("resize", update_back_position, true);
                      var iFrame         = window.frameElement;
                      var frameOffset    = get_offset(iFrame);
                      var documentBottom = parent.window.innerHeight  + parent.window.scrollY;
                      var extHeight      = Math.round(frameOffset.top + iFrame.getBoundingClientRect().height - documentBottom);
                      if (extHeight<0) { extHeight = 0; }
                      returnButton.style.bottom = (10+extHeight) + "px";
                      document.getElementById("tooltiptext").style.bottom = (11+extHeight) + "px";
                      scrollPos = contentDiv.scrollTop - 25 + iFrame.getBoundingClientRect().height - extHeight;
                  }
                  else{
                      scrollPos = window.scrollY + window.innerHeight - 25;}
                  if (returnElem.offsetTop>scrollPos){
                      document.getElementById("down").style.display = "inline";
                      document.getElementById("up").style.display   = "none";   }
                  else{
                      document.getElementById("down").style.display = "none";
                      document.getElementById("up").style.display   = "inline"; }
              }
              catch(e){}
          }
          function set_theme(themePref)
          {
            var themeSwitch     = document.getElementById("ToggleTheme");
            var themeSwitchText = "switch to";
            var switchToText    = null;
            if (!themePref){ themePref = get_theme_pref(); }
            if (themePref.localeCompare("light")===0){
                document.getElementById("dark-theme").sheet.disabled = true;
                document.getElementById("hide-dark").sheet.disabled  = false;
                switchToText = " dark theme";}
            else{
                document.getElementById("dark-theme").sheet.disabled = false;
                document.getElementById("hide-dark").sheet.disabled  = true;
                switchToText = " light theme";}
            themeSwitch.innerHTML = themeSwitchText + switchToText;
            set_theme_pref(themePref);
          }

          function toggle_theme()
          {
            if (document.getElementById("dark-theme").sheet.disabled) { set_theme("dark");  }
            else                                                      { set_theme("light"); }
          }

          function set_theme_pref(themePref)
          {
              var d = new Date();
              d.setTime(d.getTime() + (2*365*24*60*60*1000));
              var expires = "expires="+ d.toUTCString();
              document.cookie = "themepref=" + themePref + ";" + expires + "path=/";
              localStorage.setItem("PRETTY_THEME", themePref);
          }

          function get_theme_pref() {
              var name = "themepref=";
              var decodedCookie = decodeURIComponent(document.cookie);
              var ca = decodedCookie.split(';');
              for(var i = 0; i < ca.length; i++) {
                var c = ca[i];
                while (c.charAt(0) == ' ') {
                  c = c.substring(1);
                }
                if (c.indexOf(name) == 0) {
                  return c.substring(name.length, c.length);
                }
              }
              var docTheme = localStorage.getItem("PRETTY_THEME");
              if (docTheme) { return docTheme }
              else          { return "light"  }
          }

          function toggle_details(section)
          {
            var link;
            var subSection;
            var details;
            var linkText;
            var i;
            var openState  = true;
            var border     = "6px 6px 0 0;"
            if (section===0)
            {
              link = document.getElementById("Toggle"+section.toString());
              if (link.innerHTML.localeCompare("collapse all on page")===0){
                  openState = false;
                  border    = "6px;"
                  linkText  = "expand all";}
              else{
                  linkText   = "collapse all";}
              link.innerHTML = linkText + " on page";
              for (i = 0; i < allDetails.length; i++){
                 allDetails[i].open = openState;
                 allDetails[i].children[0].setAttribute( 'style', "border-radius:"+border );
                 link = document.getElementById("Toggle"+allDetails[i].id.split(".", 1));
                 if (allDetails[i].id.charAt(0).localeCompare("0") && link){link.innerHTML = linkText;}}
            }
            else
            {
               link = document.getElementById("Toggle"+section.toString());
               subSection = 1;
               if (link.innerHTML.localeCompare("collapse all")===0){
                  openState      = false;
                  border         = "6px;"
                  link.innerHTML = "expand all";}
               else{
                  link.innerHTML = "collapse all";}
               details = document.getElementById(section.toString()+"."+subSection.toString());
               while (details){
                    details.open = openState;
                    details.children[0].setAttribute( 'style', "border-radius:"+border );
                    subSection++;
                    details = document.getElementById(section.toString()+"."+subSection.toString());}
               var allCollapsed = true;
               var allExpanded  = true;
               for (i = 0; i < allDetails.length; i++){
                   check_if_open(allDetails[i]);}
               link = document.getElementById("Toggle0");
               if (allExpanded) {link.innerHTML = "collapse all on page";}
               if (allCollapsed){link.innerHTML = "expand all on page";}
            }
            function check_if_open(details)
            {
                if (details.open){allCollapsed = false;}
                else             {allExpanded  = false;}
            }
          }

          function state_check(detailsID)
          {
              // first deal with just the section
              if (event.detail){document.activeElement.blur();}
              var clickedElem   = event.target;
              if (!skipCheck && clickedElem.localName.localeCompare("summary"))
              { 
                if (!(clickedElem.closest("summary"))) { return };
              };
              var details       = document.getElementById(detailsID);
              if ( !skipCheck ) {
                  var parentID  = clickedElem.closest("details").id;
                  if (details.id.localeCompare(parentID)) { return };}
              skipCheck         = false;
              var clickedStatus = details.open;
              var section       = detailsID.split(".", 1);
              var subSection    = 1;
              var allCollapsed  = true;
              var allExpanded   = true;
              var link          = document.getElementById("Toggle"+section);
              if (clickedStatus) { details.children[0].setAttribute( 'style', "border-radius:6px;" ); }
              else               { details.children[0].setAttribute( 'style', "border-radius:6px 6px 0 0;" ); }
              if (link)
              {
                  details = document.getElementById(section+"."+subSection.toString());
                  while (details){
                    check_if_open(details);
                    subSection++;
                    details = document.getElementById(section+"."+subSection.toString());}
                  if (allExpanded) {link.innerHTML = "collapse all";}
                  if (allCollapsed){link.innerHTML = "expand all";}
              }
              // then the whole page
              allCollapsed   = true;
              allExpanded    = true;
              for (var i = 0; i < allDetails.length; i++){
                  check_if_open(allDetails[i]);}
              link = document.getElementById("Toggle0");
              if (allExpanded) {link.innerHTML = "collapse all on page";}
              if (allCollapsed){link.innerHTML = "expand all on page";}

              function check_if_open(details)
              {
                  var openStatus
                  if (detailsID.localeCompare( details.id )===0 ){openStatus = !clickedStatus;}
                  else                                           {openStatus = details.open;}
                  if (openStatus){allCollapsed = false;}
                  else           {allExpanded  = false;}
              }
          }

          function in_iFrame ()
          {
               try {
                   return window.self !== window.top;
               } catch (e) {
                   return true;
               }
          }
</script>

</head>
<body>
<div class="content">
<div id="return-link" style="display:none;" class="tooltip">
<p onclick="jump_to()">
    <span onclick="jump_to()"><span id="up">&#8679;</span><span id="down">&#8681;</span>
    <span onclick="hide_back_link()" style="padding:2px; font-size:120%;" id="jump-close"><b onclick="hide_back_link()">&times;</b></span></span>
</p>
<div id="tooltiptext">click to return
<br>(click <b>&times;</b> to hide)</div>
</div><script>document.getElementById("dark-theme").sheet.disabled = true;</script>
<h1>DeepMIB - Network panel</h1>
<!--introduction-->
<p>The upper part of DeepMIB is occupied with the <i>Network panel</i>. This panel is used to select workflow and convolutional network architecture to be used during training</p>
<p>
<img class="image-fit" src="images\DeepLearningNetwork.png" alt=""> </p>
<p>
<b>Back to</b> <a href="im_browser_product_page.html"><b>Index</b></a> <code><b>--&gt;</b></code> <a href="im_browser_user_guide.html"><b>User Guide</b></a> <code><b>--&gt;</b></code> <a href="ug_gui_menu.html"><b>Menu</b></a> <code><b>--&gt;</b></code> <a href="ug_gui_menu_tools.html"><b>Tools Menu</b></a> <code><b>--&gt;</b></code> <a href="ug_gui_menu_tools_deeplearning.html"><b>Deep learning segmentation</b></a>
</p>
<!--/introduction-->
<p style="margin:0px; line-height:0;">&nbsp;</p>
<p onclick="toggle_details(0)" class="collapse-link"><a href="javascript:void(0);" id="Toggle0">collapse all on page</a></p><h2>Contents</h2>
<div>
<ul>
<li>
<my-a onclick="jump_to()" href="#1">Workflows</my-a>
</li>
<li>
<my-a onclick="jump_to()" href="#2">2D Semantic workflow</my-a>
</li>
<li>
<my-a onclick="jump_to()" href="#3">2.5D Semantic workflow</my-a>
</li>
<li>
<my-a onclick="jump_to()" href="#4">3D Semantic workflow</my-a>
</li>
<li>
<my-a onclick="jump_to()" href="#5">2D Patch-wise workflow</my-a>
</li>
<li>
<my-a onclick="jump_to()" href="#6">Network filename</my-a>
</li>
<li>
<my-a onclick="jump_to()" href="#7">The <span class="dropdown">GPU ▼</span> dropdown</my-a>
</li>
<li>
<my-a onclick="jump_to()" href="#8">The Eye button</my-a>
</li>
</ul>
</div>
<h2 id="1">Workflows</h2>

<b><em>Start a new project with selection of the workflow</b></em>:
<br>
<ul>
<li><b>2D Semantic</b> segmentation, where 2D image pixels that belong
to the same material are clustered together.
<li><b>2.5D Semantic</b> segmentation, where 2D network architectures are used as a template to process
small (3-9 stacks) subvolumes, where only the central slice is
segmented</li>
<li><b>3D Semantic</b> segmentation, where 3D image voxels that belong
to the same material are clustered together.
<li><b>2D Patch-wise</b> segmentation, where 2D image is predicted in
blocks (patches) resulting a heavy downsampled image indicating positions
of objects of interest
</ul>

<p style="margin:0px; line-height:0;">&nbsp;</p>
<p onclick="toggle_details(1)" class="collapse-link"><a href="javascript:void(0);" id="Toggle1">collapse all</a></p><h2 id="2">2D Semantic workflow</h2>
<details open onclick="state_check('1.1')" id="1.1"><summary> <b>Application of DeepMIB for 2D semantic segmentation of mitochondria on TEM images</b> </summary>
<div class="details-div">
<p></p>
<p>
<img class="image-fit" src="images\DeepLearning_2D_semantic.jpg" alt=""> </p>

</div>
</details>
<br>
<details open onclick="state_check('1.2')" id="1.2"><summary> <b>List of available network architectures for 2D semantic segmentation</b> </summary>
<div class="details-div">
<p></p>

The following architectures are available for 2D semantic segmentation
<ul>
<li><b>2D U-net</b>, is a convolutional neural network that was
developed for biomedical image segmentation at the Computer Science
Department of the University of Freiburg, Germany. Segmentation of a 512
x 512 image takes less than a second on a modern GPU (<a
href="https://en.wikipedia.org/wiki/U-Net">Wikipedia</a>)
<br>
<b>References:</b>
<br>
- Ronneberger, O., P. Fischer, and T. Brox. "U-Net: Convolutional
Networks for Biomedical Image Segmentation." Medical Image Computing and
Computer-Assisted Intervention (MICCAI). Vol. 9351, 2015, pp.
234-241 (<a href="https://arxiv.org/abs/1505.04597">link</a>)
<br>
- Create U-Net layers for semantic segmentation (<a
href="https://se.mathworks.com/help/vision/ref/unetlayers.html">link</a>)
<br>
</li>
<li><b>2D SegNet</b>, is a convolutional network that was
developed for segmentation of normal images University of Cambridge, UK.
It is less applicable for the microscopy dataset than U-net.
<br>
<b>References:</b>
<br>
- Badrinarayanan, V., A. Kendall, and R. Cipolla. "Segnet: A Deep Convolutional
Encoder-Decoder Architecture for Image Segmentation." arXiv. Preprint arXiv:
1511.0051, 2015 (<a href="https://arxiv.org/abs/1511.00561">link</a>)
<br>
- Create SegNet layers for semantic segmentation
(<a href="https://se.mathworks.com/help/vision/ref/segnetlayers.html">link</a>)
</li>
<li><b>2D DeepLabV3 Resnet18/Resnet50/Xception/Inception-ResNet-v2</b>, (<b><em>recommended</b></em>) an efficient DeepLab v3+ convolutional neural network for
semantic image segmentation initialized with selectable base network. Suitable for large variety
of segmentation tasks. The input images can be grayscale or RGB colors.
<ul>Base networks:
<li><b>Resnet18</b> initialize DeepLabV3 using ResNet-18, a convolutional neural network that is 18 layers deep with original image input size of 224 x 224 pixels.
This is the lightest available version that is quickest and has lowest
GPU requirements. The network is intialized using a pretrained for EM or pathology
template that is automatically downloaded the first time the network is used.

<br>
<b>ResNet-18 reference:</b> 
<br>He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. "Deep residual learning for image recognition." <a href="https://ieeexplore.ieee.org/document/7780459">In Proceedings of the IEEE conference on computer vision and pattern recognition</a>, pp. 770-778. 2016
<li><b>Resnet50</b> initialize DeepLabV3 using ResNet-50, a convolutional neural network that is 50 layers deep with original image input size of 224 x 224 pixels.
The most balanced option for moderate GPU with good performance/requirements ratio. The network is intialized using a pretrained for EM or pathology
template that is automatically downloaded the first time the network is used.

<br>
<b>ResNet-50 reference:</b> 
<br>He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. "Deep residual learning for image recognition." In <a href="https://ieeexplore.ieee.org/document/7780459">Proceedings of the IEEE conference on computer vision and pattern recognition</a>, pp. 770-778. 2016
</li>
<li><b>Xception</b> [<em><b>MATLAB version of MIB only</b></em>]
initialize DeepLabV3 using Xception, a convolutional neural network that
is 71 layers deep with original image input size of 299 x 299 pixels.

<br>
<b>Xception reference:</b> 
<br>Chollet, Francois, 2017. "Xception: Deep Learning with Depthwise Separable Convolutions." <a href="https://arxiv.org/abs/1610.02357">arXiv preprint, pp.1610-02357</a>.
</li>
<li><b>Inception-ResNet-v2</b> [<em><b>MATLAB version of MIB only</b></em>] initialize DeepLabV3 using Inception-ResNet-v2, a convolutional neural network that
is 164 layers deep with original image input size of 299 x 299 pixels.
This network has high GPU requirements, but expected to provide the best results. 
<br>
<b>Inception-ResNet-v2 reference:</b> 
<br>Szegedy Christian, Sergey Ioffe, Vincent Vanhoucke, and Alexander A. Alemi. "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning." In <a href="https://dl.acm.org/doi/10.5555/3298023.3298188">AAAI</a>, vol. 4, p. 12. 2017.
</li>
</ul>

<br>
<b>Reference:</b>
<br>
-  Chen, L., Y. Zhu, G. Papandreou, F. Schroff, and H. Adam. "Encoder-Decoder
with Atrous Separable Convolution for Semantic Image Segmentation."
Computer Vision - ECCV 2018, 833-851. Munic, Germany: ECCV, 2018. (<a href="https://arxiv.org/abs/1802.02611">link</a>)
<br>
</li>
</ul>


</div>
</details>
<br>
<p> <span style="line-height:8px; display:block; vertical-align:top">
<br></span></p>
<p style="margin:0px; line-height:0;">&nbsp;</p>
<p onclick="toggle_details(2)" class="collapse-link"><a href="javascript:void(0);" id="Toggle2">collapse all</a></p><h2 id="3">2.5D Semantic workflow</h2>
<details open onclick="state_check('2.1')" id="2.1"><summary> <b>Architectures available in 2.5D workflows</b> </summary>
<div class="details-div">
<p></p>

<h2>Depth to Color</h2>
In this mode subslices are arranged as color channels for sstandard 2D
architectures. Typically, this improves the segmentation results but with
the cost of x1.4-1.6 slower training.

<br> A scheme below illustrates this:
<br>
<img class="image-fit" src="images\DeepLearningNetwork25Z2C.jpg">

<br>
Comparison of segmentation results achieved using 2.5D vs 2D
architectures:
<br>
<img class="image-fit" src="images\DeepLearning_25D_comparison.jpg">

<br>
Available Depth to color architectures (for details please check the 2D semantic segmentation section above):
<br>
<ul>
<li><b>Z2C + U-net</b>, a standard U-net architecture is used as the template</li>
<li><b>Z2C + DLv3 Resnet18</b>, DeepLab v3 architecture based on Resnet 18</li>
<li><b>Z2C + DLv3 Resnet50</b>, DeepLab v3 architecture based on Resnet 50</li>
</ul>

<br>
<h2>3D convolutions for substacks</h2>
in this mode the substacks are processed using 3D convolutions. As the 3D
convolutions are quite slow, this mode is significantly slower than 2D or
2.5 Z2C approaches.

<br>
<b>Available architectures:</b>
<ul>
<li><b>3DC + DLv3 Resnet 18</b>, an adapted for 2.5D approach DeepLab v3
architecture based on Resnet 18</li>
</ul>


</div>
</details>
<br>
<p> <span style="line-height:8px; display:block; vertical-align:top">
<br></span></p>
<details open onclick="state_check('2.2')" id="2.2"><summary> <b>Notes on application of 2.5D workflows</b> </summary>
<div class="details-div">
<p></p>
<p>
Work with the 2.5D workflows is essentially the same as with the 2D
workflows, however there
<ul>
<li><b>Input images and labels</b>, should be a small (3-9 sections) substacks, where only the central slice is segmented.
Each substack should be saved in a separate file</li>
<li><b>Preprocessing</b> is not implemented</li>
<li><b>Generation of patches for training</b>, the subvolumes for
traininig can be automatocally generated using:
<ul>
<li>From models: <em>Menu->Models(or
Masks)->Model (Mask) statistics->detect obkects->right mouse click->Crop
to a file</em></li>
<li>From annotations: <em>Menu->Models->Annotations->right mouse click over the selected
annotations->Crop out patches around selected annotations</em></li>
<li>Check this youtube video: <a href="https://youtu.be/QrKHgP76_R0?si=j_58ipCpp6Sn7r11">https://youtu.be/QrKHgP76_R0?si=j_58ipCpp6Sn7r11</a>
</p>

</div>
</details>
<br>
<p> <span style="line-height:8px; display:block; vertical-align:top">
<br></span></p>
<p style="margin:0px; line-height:0;">&nbsp;</p>
<p onclick="toggle_details(3)" class="collapse-link"><a href="javascript:void(0);" id="Toggle3">collapse all</a></p><h2 id="4">3D Semantic workflow</h2>
<p>3D Semantic workflow is suitable for anisotropic and slightly anisotropic 3D datasets, where information from multiple sections is utilized to train a network for better prediction of 3D structures.</p>
<details open onclick="state_check('3.1')" id="3.1"><summary> <b>List of available network architectures for 3D semantic segmentation</b> </summary>
<div class="details-div">
<p></p>

The following architectures are available for 3D semantic segmentation:
<ul>
<li><b>3D U-net</b>, a variation of U-net, suitable for for semantic
segmentation of volumetric images.
<br>
<b>References:</b>
<br>
- Cicek, &Ouml;., A. Abdulkadir, S. S. Lienkamp, T. Brox, and O. Ronneberger.
"3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation."
Medical Image Computing and Computer-Assisted Intervention, MICCAI 2016.
MICCAI 2016. Lecture Notes in Computer Science. Vol. 9901, pp. 424-432.
Springer, Cham (<a href="https://arxiv.org/abs/1606.06650">link</a>)
<br>
- Create 3-D U-Net layers for semantic segmentation of volumetric images
(<a href="https://se.mathworks.com/help/vision/ref/unet3dlayers.html">link</a>)
</li>
<li><b>3D U-net anisotropic</b>, a hybrid U-net that is a combination of
2D and 3D U-nets. The top layer of the network has 2D convolutions and 2D
max pooling operation, while the rest of the steps are done in 3D. As
result, it is better suited for datasets with anisotropic voxels
</ul>

<details open onclick="state_check('3.2')" id="3.2"><summary> <b>Architecture of 3D U-net anisotropic</b> </summary>
<div class="details-div">
<p></p>
<p>
<img class="image-fit" src="images\deeplearning_3d_Unet_Ani.png" alt=""> </p>

</div>
</details>
<br>

</div>
</details>
<br>
<p><span style="line-height:8px; display:block; vertical-align:top">
<br></span></p>
<p style="margin:0px; line-height:0;">&nbsp;</p>
<p onclick="toggle_details(4)" class="collapse-link"><a href="javascript:void(0);" id="Toggle4">collapse all</a></p><h2 id="5">2D Patch-wise workflow</h2>
<p>In the patch-wise workflow the training is done on patches of images, where each image contains an example of a specific class. 
<br> During prediction, the images are processed in blocks (with or without an overlap) and each block is assigned to one or another class.<span style="line-height:16px; display:block; vertical-align:top">
<br></span> This workflow may be useful to quickly find areas where the object of interest is located or to target semantic segmentation to some specific aras.</p>
<details open onclick="state_check('4.1')" id="4.1"><summary> <b>Detection of nuclei using the 2D patch-wise workflow</b> </summary>
<div class="details-div">
<p></p>
<p>Examples of patches for detection of nuclei using the 2D patch-wise workflow:</p>
<p>
<img class="image-fit" src="images\DeepLearning_2D_patchwise_patches2.png" alt=""> </p>
<p>Snapshot showing result of 2D patch-wise segmentation of nuclei.
<br></p>
<div>
<ul>
<li>Green color patches indicate predicted locations of nuclei</li>
<li>Red color patches indicate predicted locations of background</li>
<li>Uncolored areas indicate patches that were skipped due to dynamic masking</li>
</ul>
</div>
<p>
<img class="image-fit" src="images\DeepLearningPredictDynMaskingResults.png" alt=""> </p>

</div>
</details>
<br>
<details open onclick="state_check('4.2')" id="4.2"><summary> <b>Detection of spots using the 2D patch-wise workflow</b> </summary>
<div class="details-div">
<p></p>
<p>The images below show examples of patches of two classes: "spots" and "background" using for training.</p>
<p>
<img class="image-fit" src="images\DeepLearning_2D_patchwise_patches.png" alt=""> </p>
<p>Synthetic example showing detection of spots using the patch-wise workflow.</p>
<p>
<img class="image-fit" src="images\DeepLearning_2D_patchwise.png" alt=""> </p>

</div>
</details>
<br>
<details open onclick="state_check('4.3')" id="4.3"><summary> <b>List of available networks for the 2D patch-wise workflow</b> </summary>
<div class="details-div">
<p></p>
<details open onclick="state_check('4.4')" id="4.4"><summary> <b>Comparison of different network architectures</b> </summary>
<div class="details-div">
<p> Indicative plot of the relative speeds of the different networks (credit: Mathworks Inc.):</p>
<p>
<img class="image-fit" src="https://se.mathworks.com/help/deeplearning/ug/pretrained_20b.png" alt=""> </p>

</div>
</details>
<br>
<p><span class="h3">Resnet18</span> a convolutional neural network that is 18 layers deep. It is fairly light network, which is however capable to give very nice results. The default image size is 224x224 pixels but it can be adjusted to any value.
<br> This network in MIB for MATLAB can be initialized using a pretrained version of the network trained on more than a million images from the ImageNet database.</p>
<p>
<b>References</b>
</p>
<div>
<ul>
<li>ImageNet. <a href="http://www.image-net.org">http://www.image-net.org</a>
</li>
<li>He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. "Deep residual learning for image recognition." In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778. 2016.</li>
</ul>
</div>
<p><span class="h3">Resnet50</span> a convolutional neural network that is 50 layers deep. The default image size is 224x224 pixels but it can be adjusted to any value.
<br> This network in MIB for MATLAB can be initialized using a pretrained version of the network trained on more than a million images from the ImageNet database.</p>
<p>
<b>References</b>
</p>
<div>
<ul>
<li>ImageNet. <a href="http://www.image-net.org">http://www.image-net.org</a>
</li>
<li>He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. "Deep residual learning for image recognition." In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778. 2016.</li>
<li>
<a href="https://keras.io/api/applications/resnet/#resnet50-function">https://keras.io/api/applications/resnet/#resnet50-function</a>
</li>
</ul>
</div>
<p><span class="h3">Resnet101</span> a convolutional neural network that is 101 layers deep. The default image size is 224x224 pixels but it can be adjusted to any value.
<br> This network in MIB for MATLAB can be initialized using a pretrained version of the network trained on more than a million images from the ImageNet database.</p>
<p>
<b>References</b>
</p>
<div>
<ul>
<li>ImageNet. <a href="http://www.image-net.org">http://www.image-net.org</a>
</li>
<li>He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. "Deep residual learning for image recognition." In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778. 2016.</li>
<li>
<a href="https://github.com/KaimingHe/deep-residual-networks">https://github.com/KaimingHe/deep-residual-networks</a>
</li>
</ul>
</div>
<p><span class="h3">XCeption</span> is a convolutional neural network that is 71 layers deep. It is the most computationally intense network available now in DeepMIB for the patch-wise workflow. The default image size is 299x299 pixels but it can be adjusted to any value.
<br> This network in MIB for MATLAB can be initialized using a pretrained version of the network trained on more than a million images from the ImageNet database.</p>
<p>
<b>References</b>
</p>
<div>
<ul>
<li>ImageNet. <a href="http://www.image-net.org">http://www.image-net.org</a>
</li>
<li>Chollet, F., 2017. "Xception: Deep Learning with Depthwise Separable Convolutions." arXiv preprint, pp.1610-02357.</li>
</ul>
</div>

</div>
</details>
<br>
<h2 id="6">Network filename</h2>

The <b>Network filename</b> button allows to choose a file for saving the
network or for loading the pretrained network from a disk.
<br>

<br>
<ul>
<li>When the <em>Directories and preprocessing</em> or <em>Train</em> tab is selected, press of the  this
button defines a file for saving the network</li>
<li>When the <em>Predict</em> tab is selected, a user can choose a file with the
pretrained network to be used for prediction</li>
</ul>
For ease of navigation the button is color-coded to match the active tab

<p><span style="line-height:8px; display:block; vertical-align:top">
<br></span></p>
<p style="margin:0px; line-height:0;">&nbsp;</p>
<p onclick="toggle_details(5)" class="collapse-link"><a href="javascript:void(0);" id="Toggle5">collapse all</a></p><h2 id="7">The <span class="dropdown">GPU ▼</span> dropdown</h2>
<p>define execution environment for training and prediction</p>

<ul>
<li><b>Name of a GPU to use</b>, the dropdown menu starts with the list
of available GPUs to use; select one that should be used for deep learning application</li>
<li><b>Multi-GPU</b>,  use multiple GPUs on one machine, using a local parallel pool based on your default cluster profile.
If there is no current parallel pool, the software starts a parallel pool
with pool size equal to the number of available GPUs. <em>This option is
only shown when multiple GPUs are present on the system</em></li>
<li><b>CPU only</b>, do calculation using only a single available CPU</li>
<li><b>Parallel</b> (<b>under development</b>), use a local or remote parallel pool based on your default cluster profile.
If there is no current parallel pool, the software starts one using the default cluster profile.
If the pool has access to GPUs, then only workers with a unique GPU perform training computation.
If the pool does not have GPUs, then training takes place on all available CPU workers instead</li>
</ul>
Press the "<b>?</b>" button to see GPU info dialog
<br>

<details open onclick="state_check('5.1')" id="5.1"><summary> <b>GPU information dialog</b> </summary>
<div class="details-div">
<p></p>
<p>
<img class="image-fit" src="images\DeepLearningNetwork_GPUinfo.png" alt=""> </p>

</div>
</details>
<br>
<h2 id="8">The Eye button</h2>
<p>Load the network file specified in the <span class="dropdown">Network filename</span> field and perform its check.
<br> This operation differs from pressing the <span class="kbd">Check network</span> button in the 'Train' panel. When you press this button, the network stored in the file is loaded instead of generating it using the parameters in the 'Train' panel.</p>
<p>
<b>Back to</b> <a href="im_browser_product_page.html"><b>Index</b></a> <code><b>--&gt;</b></code> <a href="im_browser_user_guide.html"><b>User Guide</b></a> <code><b>--&gt;</b></code> <a href="ug_gui_menu.html"><b>Menu</b></a> <code><b>--&gt;</b></code> <a href="ug_gui_menu_tools.html"><b>Tools Menu</b></a> <code><b>--&gt;</b></code> <a href="ug_gui_menu_tools_deeplearning.html"><b>Deep learning segmentation</b></a>
</p>
<p></p>
<p>
<script>
  var allDetails = document.getElementsByTagName('details');
  toggle_details(0);
</script>
</p>
<p class="footer">

<br>
<a href="https://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2023b</a> and subsequently processed by <a class="pretty-link" href="https://www.mathworks.com/matlabcentral/fileexchange/78059-prettify-matlab-html">prettify_MATLAB_html</a> V6.8b2</p>
<p id="iFrameBuf">&nbsp;</p>
</div>
<!--
##### SOURCE BEGIN #####
%% DeepMIB - Network panel
% The upper part of DeepMIB is occupied with the _Network panel_. This
% panel is used to select workflow and convolutional network architecture to be used during training
%
% <<images\DeepLearningNetwork.png>>
% 
% *Back to* <im_browser_product_page.html *Index*> |*REPLACE_WITH_DASH_DASH>*| <im_browser_user_guide.html *User Guide*> 
% |*REPLACE_WITH_DASH_DASH>*| <ug_gui_menu.html *Menu*> |*REPLACE_WITH_DASH_DASH>*| <ug_gui_menu_tools.html *Tools
% Menu*> |*REPLACE_WITH_DASH_DASH>*| <ug_gui_menu_tools_deeplearning.html *Deep learning segmentation*>
% 
%
%% Workflows
%
% <html lang="en">
% <b><em>Start a new project with selection of the workflow</b></em>:
<br>
% <ul>
% <li><b>2D Semantic</b> segmentation, where 2D image pixels that belong
% to the same material are clustered together. 
% <li><b>2.5D Semantic</b> segmentation, where 2D network architectures are used as a template to process
% small (3-9 stacks) subvolumes, where only the central slice is
% segmented</li>
% <li><b>3D Semantic</b> segmentation, where 3D image voxels that belong
% to the same material are clustered together. 
% <li><b>2D Patch-wise</b> segmentation, where 2D image is predicted in
% blocks (patches) resulting a heavy downsampled image indicating positions
% of objects of interest
% </ul>
% </html>
%
%% 2D Semantic workflow
%
% [dtls]<summary> *Application of DeepMIB for 2D semantic segmentation of mitochondria on TEM images* </summary>
<div class="details-div">
%
% <<images\DeepLearning_2D_semantic.jpg>>
% 
%
% [/dtls]
%
% [dtls]<summary> *List of available network architectures for 2D semantic segmentation* </summary>
<div class="details-div">
%
% <html lang="en">
% The following architectures are available for 2D semantic segmentation
% <ul>
% <li><b>2D U-net</b>, is a convolutional neural network that was
% developed for biomedical image segmentation at the Computer Science
% Department of the University of Freiburg, Germany. Segmentation of a 512
% x 512 image takes less than a second on a modern GPU (<a
% href="https://en.wikipedia.org/wiki/U-Net">Wikipedia</a>)
<br>
% <b>References:</b>
<br>
% - Ronneberger, O., P. Fischer, and T. Brox. "U-Net: Convolutional
% Networks for Biomedical Image Segmentation." Medical Image Computing and
% Computer-Assisted Intervention (MICCAI). Vol. 9351, 2015, pp.
% 234-241 (<a href="https://arxiv.org/abs/1505.04597">link</a>)
<br>
% - Create U-Net layers for semantic segmentation (<a
% href="https://se.mathworks.com/help/vision/ref/unetlayers.html">link</a>)
<br>
% </li>
% <li><b>2D SegNet</b>, is a convolutional network that was
% developed for segmentation of normal images University of Cambridge, UK.
% It is less applicable for the microscopy dataset than U-net.
<br>
% <b>References:</b>
<br>
% - Badrinarayanan, V., A. Kendall, and R. Cipolla. "Segnet: A Deep Convolutional 
% Encoder-Decoder Architecture for Image Segmentation." arXiv. Preprint arXiv: 
% 1511.0051, 2015 (<a href="https://arxiv.org/abs/1511.00561">link</a>)
<br>
% - Create SegNet layers for semantic segmentation
% (<a href="https://se.mathworks.com/help/vision/ref/segnetlayers.html">link</a>)
% </li>
% <li><b>2D DeepLabV3 Resnet18/Resnet50/Xception/Inception-ResNet-v2</b>, (<b><em>recommended</b></em>) an efficient DeepLab v3+ convolutional neural network for 
% semantic image segmentation initialized with selectable base network. Suitable for large variety
% of segmentation tasks. The input images can be grayscale or RGB colors. 
% <ul>Base networks:
% <li><b>Resnet18</b> initialize DeepLabV3 using ResNet-18, a convolutional neural network that is 18 layers deep with original image input size of 224 x 224 pixels.
% This is the lightest available version that is quickest and has lowest
% GPU requirements. The network is intialized using a pretrained for EM or pathology
% template that is automatically downloaded the first time the network is used.
% 
<br>
% <b>ResNet-18 reference:</b> 
<br>He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. "Deep residual learning for image recognition." <a href="https://ieeexplore.ieee.org/document/7780459">In Proceedings of the IEEE conference on computer vision and pattern recognition</a>, pp. 770-778. 2016
% <li><b>Resnet50</b> initialize DeepLabV3 using ResNet-50, a convolutional neural network that is 50 layers deep with original image input size of 224 x 224 pixels. 
% The most balanced option for moderate GPU with good performance/requirements ratio. The network is intialized using a pretrained for EM or pathology
% template that is automatically downloaded the first time the network is used.
% 
<br> 
% <b>ResNet-50 reference:</b> 
<br>He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. "Deep residual learning for image recognition." In <a href="https://ieeexplore.ieee.org/document/7780459">Proceedings of the IEEE conference on computer vision and pattern recognition</a>, pp. 770-778. 2016
% </li>
% <li><b>Xception</b> [<em><b>MATLAB version of MIB only</b></em>]
% initialize DeepLabV3 using Xception, a convolutional neural network that
% is 71 layers deep with original image input size of 299 x 299 pixels.
% 
<br> 
% <b>Xception reference:</b> 
<br>Chollet, Francois, 2017. "Xception: Deep Learning with Depthwise Separable Convolutions." <a href="https://arxiv.org/abs/1610.02357">arXiv preprint, pp.1610-02357</a>.
% </li>
% <li><b>Inception-ResNet-v2</b> [<em><b>MATLAB version of MIB only</b></em>] initialize DeepLabV3 using Inception-ResNet-v2, a convolutional neural network that 
% is 164 layers deep with original image input size of 299 x 299 pixels.
% This network has high GPU requirements, but expected to provide the best results. 
<br> 
% <b>Inception-ResNet-v2 reference:</b> 
<br>Szegedy Christian, Sergey Ioffe, Vincent Vanhoucke, and Alexander A. Alemi. "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning." In <a href="https://dl.acm.org/doi/10.5555/3298023.3298188">AAAI</a>, vol. 4, p. 12. 2017.
% </li>
% </ul>
% 
<br>
% <b>Reference:</b>
<br>
% -  Chen, L., Y. Zhu, G. Papandreou, F. Schroff, and H. Adam. "Encoder-Decoder 
% with Atrous Separable Convolution for Semantic Image Segmentation." 
% Computer Vision - ECCV 2018, 833-851. Munic, Germany: ECCV, 2018. (<a href="https://arxiv.org/abs/1802.02611">link</a>)
<br>
% </li>
% </ul>
% </html>
%
% [/dtls]
% [br8]
%
%% 2.5D Semantic workflow
%
% [dtls]<summary> *Architectures available in 2.5D workflows* </summary>
<div class="details-div">
% 
% <html lang="en">
% <h2>Depth to Color</h2>
% In this mode subslices are arranged as color channels for sstandard 2D
% architectures. Typically, this improves the segmentation results but with
% the cost of x1.4-1.6 slower training.
% 
<br> A scheme below illustrates this:
<br>
% <img src="images\DeepLearningNetwork25Z2C.jpg">
% 
<br>
% Comparison of segmentation results achieved using 2.5D vs 2D
% architectures:
<br>
% <img src="images\DeepLearning_25D_comparison.jpg">
% 
<br>
% Available Depth to color architectures (for details please check the 2D semantic segmentation section above):
<br>
% <ul>
% <li><b>Z2C + U-net</b>, a standard U-net architecture is used as the template</li>
% <li><b>Z2C + DLv3 Resnet18</b>, DeepLab v3 architecture based on Resnet 18</li>
% <li><b>Z2C + DLv3 Resnet50</b>, DeepLab v3 architecture based on Resnet 50</li>
% </ul>
% 
<br>
% <h2>3D convolutions for substacks</h2>
% in this mode the substacks are processed using 3D convolutions. As the 3D
% convolutions are quite slow, this mode is significantly slower than 2D or
% 2.5 Z2C approaches.
% 
<br>
% <b>Available architectures:</b>
% <ul>
% <li><b>3DC + DLv3 Resnet 18</b>, an adapted for 2.5D approach DeepLab v3
% architecture based on Resnet 18</li>
% </ul>
% </html>
%
% [/dtls]
% [br8]
%
% [dtls]<summary> *Notes on application of 2.5D workflows* </summary>
<div class="details-div">
% 
% <html lang="en">
% Work with the 2.5D workflows is essentially the same as with the 2D
% workflows, however there
% <ul>
% <li><b>Input images and labels</b>, should be a small (3-9 sections) substacks, where only the central slice is segmented. 
% Each substack should be saved in a separate file</li>
% <li><b>Preprocessing</b> is not implemented</li>
% <li><b>Generation of patches for training</b>, the subvolumes for
% traininig can be automatocally generated using:
% <ul>
% <li>From models: <em>Menu->Models(or
% Masks)->Model (Mask) statistics->detect obkects->right mouse click->Crop
% to a file</em></li>
% <li>From annotations: <em>Menu->Models->Annotations->right mouse click over the selected
% annotations->Crop out patches around selected annotations</em></li>
% <li>Check this youtube video: <a href="https://youtu.be/QrKHgP76_R0?si=j_58ipCpp6Sn7r11">https://youtu.be/QrKHgP76_R0?si=j_58ipCpp6Sn7r11</a>
% </html>
%
% [/dtls]
% [br8]
%
%% 3D Semantic workflow
%
% 3D Semantic workflow is suitable for anisotropic and slightly anisotropic
% 3D datasets, where information from multiple sections is utilized to
% train a network for better prediction of 3D structures.
%
% [dtls]<summary> *List of available network architectures for 3D semantic segmentation* </summary>
<div class="details-div">
%
% <html lang="en">
% The following architectures are available for 3D semantic segmentation:
% <ul>
% <li><b>3D U-net</b>, a variation of U-net, suitable for for semantic
% segmentation of volumetric images.
<br>
% <b>References:</b>
<br>
% - Cicek, &Ouml;., A. Abdulkadir, S. S. Lienkamp, T. Brox, and O. Ronneberger. 
% "3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation." 
% Medical Image Computing and Computer-Assisted Intervention, MICCAI 2016. 
% MICCAI 2016. Lecture Notes in Computer Science. Vol. 9901, pp. 424-432. 
% Springer, Cham (<a href="https://arxiv.org/abs/1606.06650">link</a>)
<br>
% - Create 3-D U-Net layers for semantic segmentation of volumetric images 
% (<a href="https://se.mathworks.com/help/vision/ref/unet3dlayers.html">link</a>)
% </li>
% <li><b>3D U-net anisotropic</b>, a hybrid U-net that is a combination of
% 2D and 3D U-nets. The top layer of the network has 2D convolutions and 2D
% max pooling operation, while the rest of the steps are done in 3D. As
% result, it is better suited for datasets with anisotropic voxels
% </ul>
% </html>
%
% [dtls]<summary> *Architecture of 3D U-net anisotropic* </summary>
<div class="details-div">
% 
% <<images\deeplearning_3d_Unet_Ani.png>>
%
% [/dtls]
%
% [/dtls]
%
% [br8]
%
%% 2D Patch-wise workflow
% 
% In the patch-wise workflow the training is done on patches of images,
% where each image contains an example of a specific class. 
<br>
% During prediction, the images are processed in blocks (with or without an
% overlap) and each block is assigned to one or another class.[br16]
% This workflow may be useful to quickly find areas where the object of
% interest is located or to target semantic segmentation to some specific
% aras.
%
% [dtls]<summary> *Detection of nuclei using the 2D patch-wise workflow* </summary>
<div class="details-div">
%
% Examples of patches for detection of nuclei using the 2D patch-wise
% workflow:
%
% <<images\DeepLearning_2D_patchwise_patches2.png>> 
%
% Snapshot showing result of 2D patch-wise segmentation of nuclei.
<br>
%
% * Green color patches indicate predicted locations of nuclei
% * Red color patches indicate predicted locations of background
% * Uncolored areas indicate patches that were skipped due to dynamic masking
% 
% <<images\DeepLearningPredictDynMaskingResults.png>>
%
% [/dtls]
%
% [dtls]<summary> *Detection of spots using the 2D patch-wise workflow* </summary>
<div class="details-div">
%
% The images below show examples of patches of two classes: "spots" and
% "background" using for training.
%
% <<images\DeepLearning_2D_patchwise_patches.png>>
%
% Synthetic example showing detection of spots using the patch-wise
% workflow.
%
% <<images\DeepLearning_2D_patchwise.png>> 
%
% [/dtls]
%
%
% [dtls]<summary> *List of available networks for the 2D patch-wise workflow* </summary>
<div class="details-div">
%
% [dtls]<summary> *Comparison of different network architectures* </summary>
<div class="details-div">
% Indicative plot of the relative speeds of the different networks (credit:
% Mathworks Inc.):
% 
% <<https://se.mathworks.com/help/deeplearning/ug/pretrained_20b.png>>
% 
% [/dtls]
%
% [class.h3]Resnet18[/class]
% a convolutional neural network that is 18 layers deep. It is fairly light
% network, which is however capable to give very nice results. The default image size is 224x224 pixels but it can be adjusted to any value.
<br>
% This network in MIB for MATLAB can be initialized using a pretrained
% version of the network trained on more than a million images from the ImageNet database.
%
% *References* 
%
% * ImageNet. http://www.image-net.org
% * He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. "Deep residual learning for image recognition." In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778. 2016.
%
%
% [class.h3]Resnet50[/class]
% a convolutional neural network that is 50 layers deep. The default image size is 224x224 pixels but it can be adjusted to any value.
<br>
% This network in MIB for MATLAB can be initialized using a pretrained
% version of the network trained on more than a million images from the ImageNet database.
%
% *References* 
%
% * ImageNet. http://www.image-net.org
% * He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. "Deep residual learning for image recognition." In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778. 2016.
% * https://keras.io/api/applications/resnet/#resnet50-function
%
%
% [class.h3]Resnet101[/class]
% a convolutional neural network that is 101 layers deep. The default image size is 224x224 pixels but it can be adjusted to any value.
<br>
% This network in MIB for MATLAB can be initialized using a pretrained
% version of the network trained on more than a million images from the ImageNet database.
%
% *References* 
%
% * ImageNet. http://www.image-net.org
% * He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. "Deep residual learning for image recognition." In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778. 2016.
% * https://github.com/KaimingHe/deep-residual-networks
%
%
% [class.h3]XCeption[/class]
% is a convolutional neural network that is 71 layers deep. It is the most computationally intense network available now in DeepMIB for the patch-wise workflow.
% The default image size is 299x299 pixels but it can be adjusted to any value.
<br>
% This network in MIB for MATLAB can be initialized using a pretrained
% version of the network trained on more than a million images from the ImageNet database.
%
% *References* 
%
% * ImageNet. http://www.image-net.org
% * Chollet, F., 2017. "Xception: Deep Learning with Depthwise Separable Convolutions." arXiv preprint, pp.1610-02357.
%
%
% [/dtls]
%
%
%
%% Network filename
%
% <html lang="en">
% The <b>Network filename</b> button allows to choose a file for saving the
% network or for loading the pretrained network from a disk.
<br>
% 
<br>
% <ul>
% <li>When the <em>Directories and preprocessing</em> or <em>Train</em> tab is selected, press of the  this
% button defines a file for saving the network</li>
% <li>When the <em>Predict</em> tab is selected, a user can choose a file with the
% pretrained network to be used for prediction</li>
% </ul>
% For ease of navigation the button is color-coded to match the active tab
% </html>
%
% [br8]
%
%% The [class.dropdown]GPU  &#9660;[/class] dropdown
% define execution environment for training and prediction
%
% <html lang="en">
% <ul>
% <li><b>Name of a GPU to use</b>, the dropdown menu starts with the list
% of available GPUs to use; select one that should be used for deep learning application</li>
% <li><b>Multi-GPU</b>,  use multiple GPUs on one machine, using a local parallel pool based on your default cluster profile. 
% If there is no current parallel pool, the software starts a parallel pool
% with pool size equal to the number of available GPUs. <em>This option is
% only shown when multiple GPUs are present on the system</em></li>
% <li><b>CPU only</b>, do calculation using only a single available CPU</li>
% <li><b>Parallel</b> (<b>under development</b>), use a local or remote parallel pool based on your default cluster profile. 
% If there is no current parallel pool, the software starts one using the default cluster profile. 
% If the pool has access to GPUs, then only workers with a unique GPU perform training computation. 
% If the pool does not have GPUs, then training takes place on all available CPU workers instead</li>
% </ul>
% Press the "<b>?</b>" button to see GPU info dialog
<br>
% </html>
%
% [dtls]<summary> *GPU information dialog* </summary>
<div class="details-div">
%
% <<images\DeepLearningNetwork_GPUinfo.png>>
%
% [/dtls]
%
%% The Eye button
% Load the network file specified in the [class.dropdown]Network
% filename[/class] field and perform its check.
<br>
% This operation differs from pressing the [class.kbd]Check network[/class] button in the 'Train' panel. 
% When you press this button, the network stored in the file is loaded instead of generating it using the parameters in the 'Train' panel.
%
% 
% *Back to* <im_browser_product_page.html *Index*> |*REPLACE_WITH_DASH_DASH>*| <im_browser_user_guide.html *User Guide*> 
% |*REPLACE_WITH_DASH_DASH>*| <ug_gui_menu.html *Menu*> |*REPLACE_WITH_DASH_DASH>*| <ug_gui_menu_tools.html *Tools
% Menu*> |*REPLACE_WITH_DASH_DASH>*| <ug_gui_menu_tools_deeplearning.html *Deep learning segmentation*>
%
% [cssClasses]
% .dropdown { 
%   font-family: monospace;
% 	border: 1px solid #aaa; 
% 	border-radius: 0.2em; 
% 	background-color: #fff; 
% 	padding: 0.1em 0.4em; 
% 	font-size: 1em;
% }
% .kbd { 
%   font-family: monospace;
% 	border: 1px solid #aaa; 
% 	-moz-border-radius: 0.2em; 
% 	-webkit-border-radius: 0.2em; 
% 	border-radius: 0.2em; 
% 	-moz-box-shadow: 0.1em 0.2em 0.2em #ddd; 
% 	-webkit-box-shadow: 0.1em 0.2em 0.2em #ddd; 
% 	box-shadow: 0.1em 0.2em 0.2em #ddd; 
% 	background-color: #f9f9f9; 
% 	background-image: -moz-linear-gradient(top, #eee, #f9f9f9, #eee); 
% 	background-image: -o-linear-gradient(top, #eee, #f9f9f9, #eee); 
% 	background-image: -webkit-linear-gradient(top, #eee, #f9f9f9, #eee); 
% 	background-image: linear-gradient(&#91;&#91;:Template:Linear-gradient/legacy]], #eee, #f9f9f9, #eee); 
% 	padding: 0.1em 0.4em; 
% 	font-family: inherit; 
% 	font-size: 1em;
% }
% .h3 {
% color: #E65100;
% font-size: 12px;
% font-weight: bold;
% }
% .code {
% font-family: monospace;
% font-size: 10pt;
% background: #eee;
% padding: 1pt 3pt;
% }
% [/cssClasses]
%%
% <html lang="en">
% <script>
%   var allDetails = document.getElementsByTagName('details');
%   toggle_details(0);
% </script>
% </html>
##### SOURCE END #####
-->
<script>
var allDetails   = document.getElementsByTagName('details');
var contentDiv   = document.getElementsByClassName("content"); contentDiv = contentDiv[0];
var returnButton = document.getElementById("return-link");
document.getElementById("iFrameBuf").style.display = "none";
if(in_iFrame())
{
   try{
      var footerNav = parent.document.getElementsByClassName("footernav");
      var tabPane   = parent.document.getElementsByClassName("tab-pane");}
   catch(err) { var footerNav = []; var tabPane = [];};
   if(!(footerNav.length) || tabPane.length)
   {
      contentDiv.style.overflowY = "scroll";
      contentDiv.style.overflowX = "hidden";
      contentDiv.style.position  = "absolute";
      contentDiv.style.width     = "95%";
      contentDiv.style.top       = 0;
      contentDiv.style.bottom    = 0;
      if (tabPane.length){
         contentDiv.setAttribute("data-isMATLABCentral","1");
         returnButton.style.right = "40px";
         document.getElementById("tooltiptext").style.right = "92px"; }
      document.getElementById("iFrameBuf").style.display = "block";
   }
   else { contentDiv.setAttribute("data-isHelpBrowser","1"); }
}
if (!contentDiv.getAttribute("data-isHelpBrowser") && !contentDiv.getAttribute("data-isMATLABCentral") ){
   document.getElementById("anchor-offsets").sheet.disabled = true; }
</script></body>
</html>
